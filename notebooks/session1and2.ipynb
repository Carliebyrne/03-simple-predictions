{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seaborn Refresher\n",
    "\n",
    "Let's review using Seaborn and Pandas to load up some data and then pair plot it.\n",
    "\n",
    "We'll be using the same tools that we used last week for this \n",
    "- [pandas](pandas.pydata.org) for data handling (our dataframe library)\n",
    "- [seaborn](seaborn.pydata.org) for _nice_ data visualization\n",
    "\n",
    "Shortly we'll also by trying out:\n",
    "\n",
    "- [scikit-learn](scikit-learn.org) an extensive machine learning library.\n",
    "- [numpy](numpy.org) - a fundamental maths library best used by people with a strong maths background.  We won't explore it much today, but it does have some useful methods that we'll need.  It underlies all other mathematical and plotting tools that we use in Python.\n",
    "\n",
    "We'll be using scikit-learn over the next few weeks, and it's well worth reading the documentation and high level descriptions.\n",
    "\n",
    "As before, the aim is to get familiar with code-sharing workflows - so we will be doing pair programming for the duration of the day! _You will probably want to take a moment to look at the documentation of the libraries above - especially pandas_\n",
    "\n",
    "The other useful resource is Stack Overflow - if you have a question that sounds like 'how do I do {x}' then someone will probably have answered it on SO. Questions are also tagged by library so if you have a particular pandas question you can do something like going to https://stackoverflow.com/questions/tagged/pandas (just replace the 'pandas' in the URL with whatever library you're trying to use.\n",
    "\n",
    "Generally answers on SO are probably a lot closer to getting you up and running than the documentation. Once you get used to the library then the documentation is generally a quicker reference. We will cover strategies for getting help in class.\n",
    "\n",
    "## Git links\n",
    "\n",
    "We will be working through using GitHub and GitKraken to share code between pairs. We will go through all the workflow in detail in class but here are some useful links for reference:\n",
    "\n",
    "- GitKraken interface basics: https://support.gitkraken.com/start-here/interface\n",
    "- Staging and committing (save current state -> local history): https://support.gitkraken.com/working-with-commits/commits\n",
    "- Pushing and pulling (sync local history <-> GitHub history): https://support.gitkraken.com/working-with-repositories/pushing-and-pulling\n",
    "- Forking and pull requests (request to sync your GitHub history <-> someone else's history - requires a _review_):\n",
    "  - https://help.github.com/articles/about-forks/\n",
    "  - https://help.github.com/articles/creating-a-pull-request-from-a-fork/\n",
    "\n",
    "## Step 1: Read in the dataset and pairplot\n",
    "\n",
    "For this exercise, we will be using the Tips dataset that you can find in the same directory as this notebook.  This is a widely used dataset in machine learning, and while not related to minerals and energy, it is sufficient for our purpose.  The dataset relates total bills at US restaurants to tip size, as well as the sex of the tipper, whether they smoke, the day of the week, the kind of meal, and the number of people.\n",
    "\n",
    "In pairs work out how to read this data into a pandas dataframe, then use Seaborn to pairplot the species in the dataset.\n",
    "\n",
    "Seaborn happens to have this dataset built in.  Run the next cell to see the built in data.  Then modify the code to open the dataset from a CSV file.  The dataset can take a little while to load, so be patient - the dataset will appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "tips = sns.load_dataset('tips')\n",
    "tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Find a linear regression with Seaborn\n",
    "\n",
    "Now that you've seen some pairplots (tips vs tip size are the most meaningful comparisons), use Seaborn to find lines of best fit in this dataset.\n",
    "\n",
    "There are a few different ways to do this.  Try using regplot.\n",
    "\n",
    "You may notice a \"FutureWarning\".  Ignore this - Python is often in a state of flux and these types of warning are common.  Often with major packages like Seaborn you'll find that a soon-to-be-released version of the library will not create these warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Linear regression with scikit-learn\n",
    "\n",
    "Scikit-learn provides machine learning tools in several categories.  These include supervised learning and unsupervised learning.  We'll start working with unsupervised learning next week.  Supervised learning is about finding a model for features that can be measured and some labelling that we have for the available data.  If, for example, we have lithium assays and we want to try to predict lithium based on sensor data from a portable spectrometer, then the lithium assays are the labels and the measured intensities at different wavelengths are the measured features.  This kind of supervised learning is called regression.\n",
    "\n",
    "There's another kind of supervised learned which is called classification, this is what we're doing when we want to assign observed data to different discrete classes.  Regression can sometimes be used, with minor additions, to classify data as well.  For example, with our lithium spectral regression model we could classify samples as being high in lithium or low in lithium simply by using a threshold value that we set.  There are more sophisticated ways to classify, which will be covered in later weeks.\n",
    "\n",
    "We use the estimator API of scikit-learn to do regression.\n",
    "\n",
    "### The Estimator API of scikit-learn\n",
    "\n",
    "There are a few steps to follow when using the estimator API.  These steps are the same for all methods that scikit-learn implements, not just for linear regression.\n",
    "\n",
    "1. Choose a class of model by importing the appropriate estimator class.  In our case we want to import Linear Regression.  Here's how we can do it.\n",
    "\n",
    "First import LinearRegression from scikit-learn.  Use this code:\n",
    "\n",
    "```from sklearn.linear_model import LinearRegression```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create an \"instance\" of the LinearRegression class.  We can do it like this:\n",
    "\n",
    "```model = LinearRegression(fit_intercept=True)```\n",
    "\n",
    "To check that this has worked look at the model object after it's created.  It should tell you about some of its settings.\n",
    "\n",
    "```model```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These settings are also called hyperparameters.  We'll encounter hyperparameters again next week, and will talk about them in more detail then.  They're often very important in working out whether our model is well fitted to the data.\n",
    "\n",
    "2. Next we need to arrange a pandas dataframe (like \"tips\") into a features matrix and a target vector.\n",
    "\n",
    "Search on the Internet for this.  I know that Stack Overflow will be helpful.  You will need to look at the column names in the dataframe to find the names of the two columns that are important to us.  Do this in the next cell.\n",
    "\n",
    "The notation is a bit strange!  The two pairs of \"[ ]\" as \"[[ ]]\" that you will see is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Fit the model to your data by using the fit() method of the LinearRegression object.\n",
    "\n",
    "Again, look at the documentation for how to apply this.  You'll need to provide your features matrix (X) and target vector (y) as parameters to the fit method.\n",
    "\n",
    "#### Congratulations you've trained your first machine learning model!\n",
    "\n",
    "As this is a two dimensional linear model, it has two parameters.  The line's intercept and slope.  The notation that scikit-learn uses is a little unfriendly.  Its convention is to add underscores to the names of the parameters it finds.  Also, it calls the slope \"coef\".\n",
    "\n",
    "After fitting the model, find the coefficients with ```model.coef_``` and ```model._intercept_```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we've trained a model, we should make predictions!\n",
    "\n",
    "6. Make predictions!\n",
    "\n",
    "This is also more complicated with scikit-learn than it is with Seaborn.\n",
    "\n",
    "For a given, single value for a feature (ie a meal cost) we can predict a label.  For example, for a meal cost of $20, we could make a prediction with:\n",
    "\n",
    "```predicted_tip = model.predict(20)```\n",
    "\n",
    "But to find the smooth line that seaborn finds we need to explicitly tell scikit-learn that we want to do a prediction for all of the meal costs that we're interested in.  To do this we\n",
    "use a new library called \"numpy\" and a method called linspace (which is short for linear spacing).\n",
    "\n",
    "First we need to import numpy.\n",
    "\n",
    "```import numpy as np```\n",
    "\n",
    "While I used predicted_tip above as an example of a predicted target array, and 20 is an example of x, I'll now switch to the usual y and x conventions used in tutorials with scikit-learn.  You can of course use any variables names you, and in your own code it's best to use descriptive names that mean something in the domain of your industry, like 'predicted_tip\", or \"octane_rating\".\n",
    "\n",
    "We need to use the linspace method in numpy.  Use it like this:\n",
    "\n",
    "```xfit = np.linspace(0, 60)```\n",
    "\n",
    "This will create a collection of meal costs, in order, starting from 0 dollars up to 60 dollars.  This is what we need, but this collection isn't formatted correctly for scikit-learn.  To make it work with scikit-learn we next have to adjust the format with this instruction:\n",
    "\n",
    "```xfit_reshaped = xfit[:, np.newaxis]\n",
    "yfit = model.predict(xfit_reshaped).```\n",
    "\n",
    "yfit now contains our predicted tips.  Type ```yfit``` to see them numerically.\n",
    "\n",
    "Try this all out in the next cell.  Take it step by step.  Don't try to run this all in one go, but build it up line by line, checking that you do not get errors after each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot these results, but lets stop here.\n",
    "\n",
    "We could use the default plotting functions that Pandas provides for this.  But for report purposes you may, in future, want to find out how to use Seaborn for this.\n",
    "\n",
    "We can create dataframes from this data with code like:\n",
    "\n",
    "```\n",
    "DataFrame({'meal_cost':xfit, 'tips':yfit})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:  Create a linear function with scikit-learn.  Then add noise.\n",
    "\n",
    "1. Scikit-learn includes a built in function to quickly create datasets for experimenting with the estimator API.\n",
    "\n",
    "Find out about the make_regression method in sklearn.datasets.\n",
    "\n",
    "Use this to make a noisy line with 100 samples.  Use n_features to set the number of features, and use noise to adjust gaussian noise that is added.\n",
    "\n",
    "Add outliers and see how the fitting is impacted.  LinearRegression can report R^2 values.  Use the \"score\" method.  Google and Stack Overflow will help with usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Create a linear function with seaborn.  Then add noise.\n",
    "\n",
    "While we'll be using scikit-learn over the next few weeks and it's helpful to keep working with it, it's really quite painful compared to Seaborn.  Looking at residuals is an exploratory task that Seaborn is better suited to than scikit-learn.\n",
    "\n",
    "Try this code, which plots the residuals after gaussian noise is added to a simple y = x line.  Try to get the gist of how it works.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "rs = np.random.RandomState(7)\n",
    "x = rs.uniform(0, 100, 10000)\n",
    "y = x + rs.normal(0, 1, 10000)\n",
    "\n",
    "sns.residplot(x, y, lowess=True, color=\"g\")\n",
    "```\n",
    "\n",
    "The RandomState object is part of the Numpy numerical package which we won't explore in detail at this time.  It is a collection of mathematics functions which underlies all other mathematical libraries that we've been using, such as Seaborn and scikit-learn.  RandomState is used for generating random numbers from distributions.\n",
    "\n",
    "A uniform distribution means that all of the values that may be returned are equally likely.  When we throw dice we are sampling from a uniform distribution.  Here we tell Python that we want random numbers between 0 and 100, all equally likely, and we want 10000 of them.\n",
    "\n",
    "A normal (or gaussian) distribution returns values which are most likely to be near the mean, falling off symetrically to either side.  It is the \"bell\" curve that you've seen many times.  Here we say that we want the error that we add to our simple line to have a mean of zero, and a standard deviation of 1.\n",
    "\n",
    "Seaborn's residplot function plots the residuals after fitting a line to the data.  With a normal distribution we expect to see these residuals evenly scattered around zero.\n",
    "\n",
    "An example of a heavy tailed distribution is the gamma distribution.  This is often used to model failure likelihood for machines.  Unlike the normal distribution it is not symmetric.  In quality control applications it quickly peaks after a short lifetime, but then has a long tail that extends many years into the future.  This makes sense as we expect most failures to be early in the life of a machine because of manufacturing faults, after that the failure time is less predictable, but we all know of machines or gadgets that seem to last forever.  Google will quickly bring up examples of the shape.\n",
    "\n",
    "Try the code above again, but substitute ```rs.normal (0, 1, 10000)``` with ```rs.gamma(2, 2, 10000)```.\n",
    "\n",
    "How would you change this code to create a heteroscadistic error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Examine the linear dataset that you have brought\n",
    "\n",
    "Find a least-squares fit using scikit-learn, and plot the residuals.  Are the residuals gaussian?  Is there homoscedasticity?  Do you have outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Metal recovery vs %sulphur in feed\n",
    "\n",
    "In this exercise we're going to look at a typical minerals engineering problem.  We have data collected in laboratory batch floatation tests on samples taken from different parts of a base metal orebody.  It appears that there is a simple relationship between metal recovery and the percent of sulphur in the sample.  We can see that recovery is increasing with sulphur.\n",
    "\n",
    "1. Open and scatterplot the file metal_recovery_vs_sulphur.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Find the least-squares linear fit for this data, without using any data transformations.  Plot the results.  Using Pandas and Seaborn may be the easiest way to approach this.\n",
    "\n",
    "The results aren't terrible, but there are some problems.  There is a definite curve in the data and the line is unable to fit through all points.  It also poor at extrapolating.  This curve will happily predict greater than 100% recovery at feed sulphur beyond around 2%.  It's also happy to advise metal recoveries of around 40% with no feed sulphur.  That may seem reasonable to a data scientist, but domain experts will regard that as ridiculous.\n",
    "\n",
    "3. Try transforming the sulphur feed percentages before fitting.  We'd like to know if the data can be made to look more linear through a simple algebraic relationship.  Domain knowledge may help here.  The general shape of the curve suggests that there may be a power relationship here.  What happens if you regress again the square root of feed sulphur?  What kind of transformation could lead a metal recovery that is limited below 100%?  Maybe it's worth trying the reciprocal of feed sulphur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
